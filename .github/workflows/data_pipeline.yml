# .github/workflows/data_pipeline.yml
name: Automated Product Data Pipelines (CSV & XML)

on:
  schedule:
    # Runs the workflow every 3 hours (at minute 0 of every 3rd hour) in UTC.
    # This corresponds to 03:00, 06:00, 09:00, 12:00, 15:00, 18:00, 21:00, 00:00 (next day) Estonia Time (EEST / UTC+3).
    - cron: '0 */3 * * *' 
  push: 
    branches:
      - main # Or 'master', depending on your default branch name
      # This trigger will run the entire pipeline whenever you push changes to your main branch,
      # useful for testing either scraper.
  workflow_dispatch: # Allows manual trigger from GitHub Actions tab

jobs:
  run_all_pipelines: # Renamed job to reflect it runs all independent pipelines
    runs-on: ubuntu-latest
    
    permissions:
      contents: write # Crucial: Grant write permissions for the GITHUB_TOKEN for all commit steps
    
    steps:
    - name: Checkout repository code
      uses: actions/checkout@v4

    - name: Set up Python 3.9
      uses: actions/setup-python@v5
      with:
        python-version: '3.9'

    # --- Pipeline 1: Scrape for CSV (for Bannerflow via Apps Script) ---
    - name: Install Playwright for CSV scraper
      run: |
        pip install playwright
        playwright install chromium # Install the Chromium browser binaries

    - name: Run CSV scraping script (scrape_prisma.py)
      # This script fetches data from the website and saves it to prisma_products.csv
      run: python scrape_prisma.py

    - name: Commit and push updated prisma_products.csv
      # This step ensures the latest scraped CSV data is saved to the repository
      run: |
        git config user.name "GitHub Actions CSV Bot"
        git config user.email "actions@github.com"
        git add prisma_products.csv
        git commit -m "Automated: Update Prisma Market products data (CSV)" || echo "No changes to commit"
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Automatically provided by GitHub

    # --- Pipeline 2: Scrape for XML (for Cropink) ---
    # Playwright is already installed from the previous step.
    - name: Run XML generation script (generate_xml_feed.py)
      # This script now scrapes the website independently and creates the XML directly.
      run: python generate_xml_feed.py

    - name: Commit and push generated cropink_feed.xml
      # This final step saves the newly generated XML feed to the repository.
      run: |
        git config user.name "github-actions XML Bot"
        git config user.email "actions@github.com"
        git add cropink_feed.xml
        git commit -m "Automated: Generate Cropink XML feed (scraped directly)" || echo "No changes to commit"
        git push
      env:
        GITHUB_TOKEN: ${{ secrets.GITHUB_TOKEN }} # Automatically provided by GitHub
